{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordTokenizers.jl\n",
    "\n",
    "https://github.com/JuliaText/WordTokenizers.jl\n",
    "\n",
    "Notebooks at https://github.com/Ayushk4/JuliaCon20_Talk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using WordTokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenizer (Sentence Splitters / segmenters)\n",
    "Uses a rule based approach with a large list of exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Dr. Jane Doe says leatherback sea turtle is the largest, measuring six or seven feet (2 m) in length at maturity, and three to five feet (1 to 1.5 m) in width, weighing up to 2000 pounds (about 900 kg). Most other species are smaller, being two to four feet in length (0.5 to 1 m) and proportionally less wide. The Flatback turtle is found solely on the northerncoast of Australia.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_sentences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizers\n",
    "These Tokenizers assume that Sentence Splitting has already been done.\n",
    "- Poorman's tokenizer\n",
    "- Punctuation space tokenize\n",
    "- Penn Tokenizer\n",
    "- Improved Penn Tokenizer\n",
    "- NLTK Word tokenizer\n",
    "- Reversible Tokenizer\n",
    "- TokTok Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"@JohnDoe says: Well, we couldn't have this cliche-ridden, \\\"Touched by Angel\\\" to check out #GitHub https://github.com.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of NLTK's default tokenizer\n",
    "print(WordTokenizers.nltk_word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of Jon Safari's toktok tokenizer\n",
    "print(WordTokenizers.toktok_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of NLTK's casual tokenizer\n",
    "print(WordTokenizers.tweet_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained using google translate\n",
    "spanish_text = \"@JohnDoe dice: Bueno, no podríamos tener este cliché, \\\"Tocado por Angel\\\" para ver #GitHub https://github.com.\"\n",
    "print(WordTokenizers.toktok_tokenize(spanish_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained using google translate\n",
    "hindi_text = \"@ जॉनडे कहते हैं: ठीक है, हम इस क्लिज्ड, \\\"एंजेल द्वारा छुआ\\\" #GitHub https://github.com की जांच कर सकते हैं।\"\n",
    "print(WordTokenizers.toktok_tokenize(hindi_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token-Buffer API\n",
    "TokenBuffer API and supporting utility lexers enables high-speed non-regex based tokenization.\n",
    "\n",
    "TokenBuffer turns a string into a readable stream, used for building tokenizers. Utility lexers such as spaces and number read characters from the stream and into an array of tokens.\n",
    "\n",
    "Lexers return true or false to indicate whether they matched in the input stream and therefore can be combined easily - \n",
    "`spacesornumber(ts) = spaces(ts) || number(ts)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using WordTokenizers: TokenBuffer, isdone, character, spaces, nltk_url1, nltk_url2, nltk_phonenumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_tok(input)\n",
    "   urls(ts) = nltk_url1(ts) || nltk_url2(ts)\n",
    "\n",
    "   ts = TokenBuffer(input)\n",
    "   while !isdone(ts)\n",
    "       spaces(ts) && continue\n",
    "       urls(ts) ||\n",
    "       nltk_phonenumbers(ts) ||\n",
    "       character(ts)\n",
    "   end\n",
    "   return ts.tokens\n",
    "end\n",
    "\n",
    "my_tok(\"A url https://github.com/JuliaText/WordTokenizers.jl/ and phonenumber +0 (987) - 2344321 #MeaningLess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical (and subword) Tokenizers\n",
    "More on this later.\n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
